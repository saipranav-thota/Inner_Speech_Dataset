{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split EEG Signals into Individual Channels\n",
    "\n",
    "This notebook takes the processed EEG epochs and splits them into individual channel files.\n",
    "\n",
    "**Input**: `processed_epochs/` folder with processed MNE epoch files  \n",
    "**Output**: `individual_channels/` folder with separate files for each channel\n",
    "\n",
    "## Output Structure:\n",
    "```\n",
    "individual_channels/\n",
    "├── by_epoch/\n",
    "│   ├── sub-01_ses-01_trial_000_Inner_Up/\n",
    "│   │   ├── A1.npy\n",
    "│   │   ├── A2.npy\n",
    "│   │   └── ... (128 channel files)\n",
    "│   └── ...\n",
    "├── by_channel/\n",
    "│   ├── A1/\n",
    "│   │   ├── sub-01_ses-01_trial_000_Inner_Up.npy\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "└── metadata/\n",
    "    ├── channel_files_metadata.csv\n",
    "    └── split_summary.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Configure settings\n",
    "mne.set_log_level('WARNING')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_PATH = \"processed_epochs\"  # Input folder with processed epochs\n",
    "OUTPUT_PATH = \"individual_channels\"  # Output folder for channel files\n",
    "\n",
    "# Create output directory structure\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_PATH}/by_epoch\", exist_ok=True)      # Organized by epoch\n",
    "os.makedirs(f\"{OUTPUT_PATH}/by_channel\", exist_ok=True)    # Organized by channel\n",
    "os.makedirs(f\"{OUTPUT_PATH}/metadata\", exist_ok=True)      # Metadata files\n",
    "\n",
    "print(f\"Input: {INPUT_PATH}\")\n",
    "print(f\"Output: {OUTPUT_PATH}\")\n",
    "print(f\"\\nOutput structure:\")\n",
    "print(f\"  {OUTPUT_PATH}/\")\n",
    "print(f\"    ├── by_epoch/     (folders per epoch, files per channel)\")\n",
    "print(f\"    ├── by_channel/   (folders per channel, files per epoch)\")\n",
    "print(f\"    └── metadata/     (CSV files and summaries)\")\n",
    "\n",
    "# Check input exists\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    print(f\"\\n❌ ERROR: {INPUT_PATH} not found!\")\n",
    "    print(\"Run the epoch processing notebook first.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Input directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed metadata\n",
    "metadata_file = f\"{INPUT_PATH}/metadata/processed_epochs_metadata.pkl\"\n",
    "\n",
    "if os.path.exists(metadata_file):\n",
    "    processed_metadata = pd.read_pickle(metadata_file)\n",
    "    print(f\"Loaded metadata for {len(processed_metadata)} processed epochs\")\n",
    "    print(f\"Duration: {processed_metadata['duration_seconds'].iloc[0]:.2f}s\")\n",
    "    print(f\"Time points: {processed_metadata['n_timepoints'].iloc[0]}\")\n",
    "    print(f\"Channels: {processed_metadata['n_channels'].iloc[0]}\")\n",
    "    print(f\"Sampling frequency: {processed_metadata['sampling_frequency'].iloc[0]} Hz\")\n",
    "else:\n",
    "    print(f\"❌ ERROR: Processed metadata not found at {metadata_file}\")\n",
    "    raise FileNotFoundError(\"Run epoch processing first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel names from first epoch\n",
    "sample_epoch_file = processed_metadata['file_path'].iloc[0]\n",
    "sample_epoch = mne.read_epochs(sample_epoch_file, verbose=False)\n",
    "channel_names = sample_epoch.ch_names\n",
    "n_channels = len(channel_names)\n",
    "n_timepoints = sample_epoch.get_data().shape[2]\n",
    "sfreq = sample_epoch.info['sfreq']\n",
    "\n",
    "print(f\"Channel information:\")\n",
    "print(f\"  Total channels: {n_channels}\")\n",
    "print(f\"  Time points per channel: {n_timepoints}\")\n",
    "print(f\"  Sampling frequency: {sfreq} Hz\")\n",
    "print(f\"  First 10 channels: {channel_names[:10]}\")\n",
    "print(f\"  Last 10 channels: {channel_names[-10:]}\")\n",
    "\n",
    "# Create channel directories\n",
    "for channel in channel_names:\n",
    "    os.makedirs(f\"{OUTPUT_PATH}/by_channel/{channel}\", exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Created {n_channels} channel directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_epoch_channels(epoch_file, epoch_id, output_base_path):\n",
    "    \"\"\"\n",
    "    Split a single epoch into individual channel files.\n",
    "    \n",
    "    Parameters:\n",
    "    - epoch_file: Path to the epoch .fif file\n",
    "    - epoch_id: Unique identifier for the epoch\n",
    "    - output_base_path: Base output directory\n",
    "    \n",
    "    Returns:\n",
    "    - success: Boolean\n",
    "    - channel_files: List of created channel file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load epoch\n",
    "        epoch = mne.read_epochs(epoch_file, verbose=False)\n",
    "        data = epoch.get_data()  # Shape: (1, n_channels, n_timepoints)\n",
    "        \n",
    "        # Remove the epoch dimension (we know it's 1)\n",
    "        data = data[0]  # Shape: (n_channels, n_timepoints)\n",
    "        \n",
    "        channel_files = []\n",
    "        \n",
    "        # Create epoch directory\n",
    "        epoch_dir = f\"{output_base_path}/by_epoch/{epoch_id}\"\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        \n",
    "        # Split into individual channels\n",
    "        for ch_idx, channel_name in enumerate(epoch.ch_names):\n",
    "            channel_data = data[ch_idx]  # Shape: (n_timepoints,)\n",
    "            \n",
    "            # Save in by_epoch structure\n",
    "            epoch_channel_file = f\"{epoch_dir}/{channel_name}.npy\"\n",
    "            np.save(epoch_channel_file, channel_data)\n",
    "            \n",
    "            # Save in by_channel structure\n",
    "            channel_epoch_file = f\"{output_base_path}/by_channel/{channel_name}/{epoch_id}.npy\"\n",
    "            np.save(channel_epoch_file, channel_data)\n",
    "            \n",
    "            channel_files.append({\n",
    "                'epoch_id': epoch_id,\n",
    "                'channel_name': channel_name,\n",
    "                'channel_index': ch_idx,\n",
    "                'by_epoch_path': epoch_channel_file,\n",
    "                'by_channel_path': channel_epoch_file,\n",
    "                'data_shape': channel_data.shape,\n",
    "                'data_min': float(channel_data.min()),\n",
    "                'data_max': float(channel_data.max()),\n",
    "                'data_mean': float(channel_data.mean()),\n",
    "                'data_std': float(channel_data.std())\n",
    "            })\n",
    "        \n",
    "        return True, channel_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {epoch_file}: {e}\")\n",
    "        return False, []\n",
    "\n",
    "# Process all epochs\n",
    "print(f\"\\nSplitting {len(processed_metadata)} epochs into individual channels...\")\n",
    "print(f\"This will create {len(processed_metadata) * n_channels} channel files...\")\n",
    "\n",
    "all_channel_files = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for idx, row in tqdm(processed_metadata.iterrows(), total=len(processed_metadata), desc=\"Splitting channels\"):\n",
    "    epoch_file = row['file_path']\n",
    "    epoch_id = row['epoch_id']\n",
    "    \n",
    "    success, channel_files = split_epoch_channels(epoch_file, epoch_id, OUTPUT_PATH)\n",
    "    \n",
    "    if success:\n",
    "        # Add epoch metadata to each channel file record\n",
    "        for channel_file in channel_files:\n",
    "            channel_file.update({\n",
    "                'subject_name': row['subject_name'],\n",
    "                'subject_number': row['subject_number'],\n",
    "                'session_number': row['session_number'],\n",
    "                'trial_number': row['trial_number'],\n",
    "                'speech_type': row['speech_type'],\n",
    "                'class': row['class'],\n",
    "                'class_id': row['class_id'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'sampling_frequency': row['sampling_frequency'],\n",
    "                'duration_seconds': row['duration_seconds'],\n",
    "                'time_start': row['time_start'],\n",
    "                'time_end': row['time_end'],\n",
    "                'freq_low': row['freq_low'],\n",
    "                'freq_high': row['freq_high']\n",
    "            })\n",
    "        \n",
    "        all_channel_files.extend(channel_files)\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "print(f\"\\n=== CHANNEL SPLITTING COMPLETE ===\")\n",
    "print(f\"✓ Successfully processed: {success_count} epochs\")\n",
    "print(f\"✗ Failed: {fail_count} epochs\")\n",
    "print(f\"✓ Total channel files created: {len(all_channel_files)}\")\n",
    "print(f\"Success rate: {success_count/(success_count+fail_count)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if len(all_channel_files) > 0:\n",
    "    channel_df = pd.DataFrame(all_channel_files)\n",
    "    \n",
    "    print(\"=== CHANNEL SPLITTING RESULTS ===\")\n",
    "    print(f\"Total channel files: {len(channel_df)}\")\n",
    "    print(f\"Unique epochs: {channel_df['epoch_id'].nunique()}\")\n",
    "    print(f\"Unique channels: {channel_df['channel_name'].nunique()}\")\n",
    "    print(f\"Files per epoch: {len(channel_df) // channel_df['epoch_id'].nunique()}\")\n",
    "    print(f\"Files per channel: {len(channel_df) // channel_df['channel_name'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n=== DATA STATISTICS ===\")\n",
    "    print(f\"Data shape per channel: {channel_df['data_shape'].iloc[0]}\")\n",
    "    print(f\"Data range: {channel_df['data_min'].min():.2e} to {channel_df['data_max'].max():.2e}\")\n",
    "    print(f\"Average mean: {channel_df['data_mean'].mean():.2e}\")\n",
    "    print(f\"Average std: {channel_df['data_std'].mean():.2e}\")\n",
    "    \n",
    "    print(\"\\n=== DISTRIBUTION CHECK ===\")\n",
    "    print(\"Files per speech type:\")\n",
    "    speech_counts = channel_df['speech_type'].value_counts()\n",
    "    for speech_type, count in speech_counts.items():\n",
    "        print(f\"  {speech_type}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "    print(\"\\nFiles per class:\")\n",
    "    class_counts = channel_df['class'].value_counts()\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "    print(\"\\nFiles per subject:\")\n",
    "    subject_counts = channel_df['subject_number'].value_counts().sort_index()\n",
    "    for subject, count in subject_counts.head(5).items():\n",
    "        print(f\"  Subject {subject}: {count} files ({count//n_channels} epochs)\")\n",
    "    \n",
    "    print(\"\\nSample channel files:\")\n",
    "    for i, (_, row) in enumerate(channel_df.head(5).iterrows()):\n",
    "        print(f\"  {i+1}. {row['epoch_id']} - {row['channel_name']}: {row['by_epoch_path']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No channel files were created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading individual channel files\n",
    "if len(all_channel_files) > 0:\n",
    "    print(\"=== TESTING CHANNEL FILES ===\")\n",
    "    \n",
    "    # Test first 3 channel files\n",
    "    test_files = channel_df.head(3)\n",
    "    \n",
    "    for i, (_, row) in enumerate(test_files.iterrows()):\n",
    "        print(f\"\\nTest {i+1}: {row['epoch_id']} - {row['channel_name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test by_epoch file\n",
    "            data_epoch = np.load(row['by_epoch_path'])\n",
    "            print(f\"  ✓ by_epoch file loaded: {data_epoch.shape}\")\n",
    "            \n",
    "            # Test by_channel file\n",
    "            data_channel = np.load(row['by_channel_path'])\n",
    "            print(f\"  ✓ by_channel file loaded: {data_channel.shape}\")\n",
    "            \n",
    "            # Verify they're identical\n",
    "            if np.array_equal(data_epoch, data_channel):\n",
    "                print(f\"  ✓ Files are identical\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Files differ!\")\n",
    "            \n",
    "            print(f\"  Data range: {data_epoch.min():.2e} to {data_epoch.max():.2e}\")\n",
    "            print(f\"  Duration: {len(data_epoch) / row['sampling_frequency']:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading files: {e}\")\n",
    "            \n",
    "    # Test loading all files for one channel\n",
    "    print(f\"\\n=== TESTING CHANNEL COLLECTION ===\")\n",
    "    test_channel = channel_names[0]  # First channel\n",
    "    channel_files = channel_df[channel_df['channel_name'] == test_channel]\n",
    "    \n",
    "    print(f\"Testing channel '{test_channel}' with {len(channel_files)} files\")\n",
    "    \n",
    "    # Load first 3 files for this channel\n",
    "    for i, (_, row) in enumerate(channel_files.head(3).iterrows()):\n",
    "        try:\n",
    "            data = np.load(row['by_channel_path'])\n",
    "            print(f\"  File {i+1}: {row['epoch_id']} - Shape: {data.shape}, Range: {data.min():.2e} to {data.max():.2e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  File {i+1}: Error - {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No channel files to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata and create summary\n",
    "if len(all_channel_files) > 0:\n",
    "    # Save channel files metadata\n",
    "    csv_file = f\"{OUTPUT_PATH}/metadata/channel_files_metadata.csv\"\n",
    "    pkl_file = f\"{OUTPUT_PATH}/metadata/channel_files_metadata.pkl\"\n",
    "    summary_file = f\"{OUTPUT_PATH}/metadata/split_summary.txt\"\n",
    "    \n",
    "    channel_df.to_csv(csv_file, index=False)\n",
    "    channel_df.to_pickle(pkl_file)\n",
    "    \n",
    "    # Create summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"CHANNEL SPLITTING SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Input: {INPUT_PATH}\\n\")\n",
    "        f.write(f\"Output: {OUTPUT_PATH}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Results:\\n\")\n",
    "        f.write(f\"  Successfully processed epochs: {success_count}\\n\")\n",
    "        f.write(f\"  Failed epochs: {fail_count}\\n\")\n",
    "        f.write(f\"  Total channel files created: {len(channel_df)}\\n\")\n",
    "        f.write(f\"  Success rate: {success_count/(success_count+fail_count)*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Dataset Info:\\n\")\n",
    "        f.write(f\"  Unique epochs: {channel_df['epoch_id'].nunique()}\\n\")\n",
    "        f.write(f\"  Unique channels: {channel_df['channel_name'].nunique()}\\n\")\n",
    "        f.write(f\"  Files per epoch: {len(channel_df) // channel_df['epoch_id'].nunique()}\\n\")\n",
    "        f.write(f\"  Files per channel: {len(channel_df) // channel_df['channel_name'].nunique()}\\n\")\n",
    "        f.write(f\"  Data shape per file: {channel_df['data_shape'].iloc[0]}\\n\")\n",
    "        f.write(f\"  Sampling frequency: {channel_df['sampling_frequency'].iloc[0]} Hz\\n\")\n",
    "        f.write(f\"  Duration per file: {channel_df['duration_seconds'].iloc[0]} seconds\\n\\n\")\n",
    "        \n",
    "        f.write(f\"File Organization:\\n\")\n",
    "        f.write(f\"  by_epoch/: {channel_df['epoch_id'].nunique()} folders, {n_channels} files each\\n\")\n",
    "        f.write(f\"  by_channel/: {n_channels} folders, {channel_df['epoch_id'].nunique()} files each\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Speech Type Distribution:\\n\")\n",
    "        for speech_type, count in channel_df['speech_type'].value_counts().items():\n",
    "            f.write(f\"  {speech_type}: {count} files ({count//n_channels} epochs)\\n\")\n",
    "        \n",
    "        f.write(f\"\\nClass Distribution:\\n\")\n",
    "        for class_name, count in channel_df['class'].value_counts().items():\n",
    "            f.write(f\"  {class_name}: {count} files ({count//n_channels} epochs)\\n\")\n",
    "        \n",
    "        f.write(f\"\\nChannel Names:\\n\")\n",
    "        for i, channel in enumerate(channel_names):\n",
    "            if i < 10 or i >= len(channel_names) - 10:\n",
    "                f.write(f\"  {channel}\\n\")\n",
    "            elif i == 10:\n",
    "                f.write(f\"  ... ({len(channel_names) - 20} more channels) ...\\n\")\n",
    "    \n",
    "    print(f\"\\n=== FILES SAVED ===\")\n",
    "    print(f\"✓ {len(channel_df)} channel files (.npy format)\")\n",
    "    print(f\"✓ Metadata: {csv_file}\")\n",
    "    print(f\"✓ Metadata: {pkl_file}\")\n",
    "    print(f\"✓ Summary: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\n=== USAGE EXAMPLES ===\")\n",
    "    print(f\"\\n1. Load single channel file:\")\n",
    "    print(f\"   data = np.load('individual_channels/by_epoch/epoch_id/channel.npy')\")\n",
    "    print(f\"   # Shape: ({n_timepoints},) - time series for one channel\")\n",
    "    \n",
    "    print(f\"\\n2. Load all channels for one epoch:\")\n",
    "    print(f\"   epoch_folder = 'individual_channels/by_epoch/sub-01_ses-01_trial_000_Inner_Up/'\")\n",
    "    print(f\"   channels = {{}}\")\n",
    "    print(f\"   for ch in channel_names:\")\n",
    "    print(f\"       channels[ch] = np.load(f'{{epoch_folder}}/{{ch}}.npy')\")\n",
    "    \n",
    "    print(f\"\\n3. Load all epochs for one channel:\")\n",
    "    print(f\"   channel_folder = 'individual_channels/by_channel/A1/'\")\n",
    "    print(f\"   epochs = []\")\n",
    "    print(f\"   for file in os.listdir(channel_folder):\")\n",
    "    print(f\"       epochs.append(np.load(f'{{channel_folder}}/{{file}}'))\")\n",
    "    \n",
    "    print(f\"\\n4. Use metadata for filtering:\")\n",
    "    print(f\"   metadata = pd.read_csv('{csv_file}')\")\n",
    "    print(f\"   inner_speech_A1 = metadata[\")\n",
    "    print(f\"       (metadata['speech_type'] == 'Inner') & \")\n",
    "    print(f\"       (metadata['channel_name'] == 'A1')\")\n",
    "    print(f\"   ]\")\n",
    "    \n",
    "    print(f\"\\n🎉 CHANNEL SPLITTING COMPLETE!\")\n",
    "    print(f\"Your data is now organized as:\")\n",
    "    print(f\"  • {len(channel_df)} individual channel files\")\n",
    "    print(f\"  • {channel_df['epoch_id'].nunique()} epochs × {n_channels} channels\")\n",
    "    print(f\"  • Organized by epoch AND by channel\")\n",
    "    print(f\"  • Each file: {n_timepoints} time points, {channel_df['duration_seconds'].iloc[0]:.2f}s duration\")\n",
    "    print(f\"  • Ready for channel-specific analysis in: {OUTPUT_PATH}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No channel files to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}